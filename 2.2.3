# Optional generic preliminaries:
graphics.off() # This closes all of R's graphics windows.
rm(list=ls())  # Careful! This clears all of R's memory!

# Required packages for this exercise (you may add more if you want to).
require(rjags)
require(coda)

# THE DATA
n = 40
k = c(19, 20, 16, 23, 22, 30, 38, 29, 34, 35, 35, 32, 37, 36, 33)
p = length(k)


# THE MODEL
exammodel2.string = "
  model {
    ## Prior
    mu <- 0.6
    kappa <- 20

    a <- mu*(kappa-2)+1
    b <- (1-mu)*(kappa-2)+1

    phi ~ dbeta(a,b)
    psi = 0.5
    for (i in 1:p) {
    z[i] ~ dbern(0.5)
      theta[i] <- equals(z[i],0)*phi + equals(z[i],1)*0.5
    }

    ## Likelihood
    for (i in 1:p){
    k[i] ~ dbin(theta[i], n)
    }
  }
"

# JAGS usually reads models from a text file; here we use a string as a fake file.
exammodel2.spec = textConnection(exammodel2.string)

# SAMPLING PARAMETERS
mcmciterations = 1000

# Construct the object containing both the model specification as well as the data and some sampling parameters.
jagsmodel2 <- jags.model(exammodel2.spec,
                         data = list('k' = k,
                                     'n' = n,
                                     'p' = p),
                         n.chains = 4)

# Collect samples to approximate the posterior distribution.
model2samples = coda.samples(jagsmodel2,
                           c('phi'), # which variables do you want to monitor?
                           n.iter = mcmciterations)


# Add your analyses on the collected samples here:
plotPost(model2samples[,'phi'])
diagMCMC(codaObject = model2samples)

summary_statistics = summary(model2samples)
